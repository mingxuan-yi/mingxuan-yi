<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>On the Probability Flow ODE of Langevin Dynamics | Mingxuan Yi</title> <meta name="author" content="Mingxuan Yi"> <meta name="description" content="an example of GANs without generators"> <meta name="keywords" content="Machine learning, Statistical computing, Artifical intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mingxuan-yi.github.io/blog/2023/prob-flow-ode/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mingxuan </span>Yi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">On the Probability Flow ODE of Langevin Dynamics</h1> <p class="post-meta">November 5, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#1-introduction">1. Introduction</a></li> <li class="toc-entry toc-h2"> <a href="#2-langevin-dynamics-and-its-probability-flow-ode">2. Langevin dynamics and its probability flow ODE</a> <ul> <li class="toc-entry toc-h4"><a href="#21-probability-flow-odes">2.1. Probability flow ODEs</a></li> <li class="toc-entry toc-h4"><a href="#22-estimating-the-vector-field-via-the-binary-classification">2.2. Estimating the vector field via the binary classification</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#3-modifying-the-code-of-gans">3. Modifying the code of GANs</a></li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="1-introduction">1. Introduction</h2> <p>This post provides a simple numerical approach using <a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">PyTorch</a> to simulate the probability flow ordinary differential equation (ODE) of Langevin dynamics. The implementation is super simple, one just needs to slightly modify a code of Generative Adversarial Nets. Such an implementation can be understood as ‘‘non-parametric GANs’’, which is an alternative view via the probability flow ODE, more details can be found in my paper ‘‘<a href="https://arxiv.org/abs/2302.01075" rel="external nofollow noopener" target="_blank">MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows</a>’’. Briefly speaking, we can remove the generator in GANs and simulate a probability flow ODE similar to diffusion models.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/blog_pic/funnel_true.png" class="img-fluid rounded z-depth-1" style="width: 110%; height: auto;"> <figcaption style="text-align: center; margin-top: 10px;"> Log density plot of Neal's funnel distribution.</figcaption> </div> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/blog_pic/funnel_langevin.gif" class="img-fluid rounded z-depth-1" style="width: 110%; height: auto;"> <figcaption style="text-align: center; margin-top: 10px;"> Langevin dynamics. <a target="_blank" href="https://colab.research.google.com/github/mingxuan-yi/prob_flow_ode/blob/main/funnel_langevin.ipynb" rel="external nofollow noopener"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> </figcaption> </div> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/blog_pic/funnel_ode.gif" class="img-fluid rounded z-depth-1" style="width: 110%; height: auto;"> <figcaption style="text-align: center; margin-top: 10px;"> Probability flow ODE. <a target="_blank" href="https://colab.research.google.com/github/mingxuan-yi/prob_flow_ode/blob/main/funnel_prob_ode.ipynb" rel="external nofollow noopener"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> </figcaption> </div> </div> <p><br> The above demo uses a modified version of Radford Neal’s <a href="https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html" rel="external nofollow noopener" target="_blank">funnel distribution</a>. The funnel distribution is known to be difficult to sample from due to its irregular geometric properties. Langevin dynamics utilizes the log density information to explore the distribution but its particles fail to reach the bottom area of the funnel. The probability flow ODE succeeds in this task as it does not rely on the density function but on samples drawn from the target distribution.</p> <p>All jupyter notebooks can also be found on <a href="https://github.com/mingxuan-yi/prob_flow_ode" rel="external nofollow noopener" target="_blank"> <img src="https://cloud.google.com/ml-engine/images/github-logo-32px.png" alt="GitHub logo" style="width: 3%; height: auto;"> </a>.</p> <p>Additionally, the paper ‘‘<a href="https://arxiv.org/abs/2305.16150" rel="external nofollow noopener" target="_blank">Unifying GANs and Score-Based Diffusion as Generative Particle Models</a>’’ and ‘‘<a href="https://arxiv.org/abs/2306.01654" rel="external nofollow noopener" target="_blank">GANs Settle Scores!</a>’’ provide explorations of non parametric versions of Integral Probability Metric GANs.</p> <h2 id="2-langevin-dynamics-and-its-probability-flow-ode">2. Langevin dynamics and its probability flow ODE</h2> <p>Langevin dynamics follows a stochastic differential equation (SDE) to describe the motion of a particle \(\mathbf{x}_t \in \mathbb{R}^n\), \begin{aligned} \mathrm{d} \mathbf{x}_t = \nabla_\mathbf{x} \log p(\mathbf{x}_t)\mathrm{d}t + \sqrt{2} \mathrm{d}\mathbf{w}_t, \end{aligned} where \(\mathbf{w}_t\) represents the Brownian motion. Using the Itô integration, we can obtain the Fokker-Placnk equation describing the marginal laws of the dynamics over time, \begin{aligned} \frac{\partial q_t(\mathbf{x})}{\partial t} = \text{div}\Big[ q_t(\mathbf{x})\big(\nabla_\mathbf{x} \log q_t(\mathbf{x}) - \nabla_\mathbf{x} \log p(\mathbf{x}) \big) \Big], \end{aligned} where \(\text{div}\) is the <a href="https://en.wikipedia.org/wiki/Divergence" rel="external nofollow noopener" target="_blank">divergence operator</a> in vector calculus.</p> <p>If the target distribution decays at infinity \(\lim_{\mathbf{x} \to \infty} p(\mathbf{x})= 0\), e.g., the Boltzmann distribution \(p(\mathbf{x}) \propto \exp\big(-V(\mathbf{x})\big)\), the equilibrium (steady state) of the dynamics is achieved if and only if \(q_t=p\) such that the infinitesimal change of the marginal is \(\frac{\partial q_t}{\partial t}=0\). Evolving a particle from the initialization \(\mathbf{x}_0 \sim q_0(\mathbf{x})\), its marginal \(q_t(\mathbf{x})\) will eventually converge (weakly) to the stationary distribution \(p(\mathbf{x})\). However, establishing the convergence rate within finite-time can be challenging; additional conditions for the target distribution must be met to guarantee convergence. For example, if \(p(\mathbf{x})\) satisfies the <a href="https://en.wikipedia.org/wiki/Logarithmic_Sobolev_inequalities" rel="external nofollow noopener" target="_blank">log-Sobolev inequality</a>, then the marginal \(q_t(\mathbf{x})\) will converge to \(p(\mathbf{x})\) exponentially fast in terms of the Kullback-Leibler divergence. Nevertheless, we shall be able to expect that Langevin dynamics can at least find some local modes in practice.</p> <p>In order to numerically simulate Langevin dynamics, we can use the Euler-Maruyama method to discretize the SDE, this gives the well-known <strong>unadjusted Langevin algorithm (ULA)</strong>, \begin{aligned} \mathbf{x}_{i+1} \leftarrow \mathbf{x}_{i} + \epsilon \nabla_{\mathbf{x}} \log p(\mathbf{x}_i) + \sqrt{2\epsilon} \mathcal{N}(0, I), \quad i=0, 1, 2\cdots \end{aligned} Iteratively running ULA, we can gradually transport samples from \(q_0(\mathbf{x})\) to \(p(\mathbf{x})\) as shown in the previous demo to sample from the funnel distribution. ULA is widely used in large-scale machine learning. For example, it can be applied in the training of Bayesian neural networks and energy-based models, and it also serves as the sampling scheme for the earliest version of score-based diffusion models (<a href="https://arxiv.org/abs/1907.05600" rel="external nofollow noopener" target="_blank">Song and Ermon, 2019</a>).</p> <h4 id="21-probability-flow-odes">2.1. Probability flow ODEs</h4> <p>There are several approaches to derive the probability flow ODE of Langevin dynamics. The most accessible one is using the result from <a href="https://yang-song.net/blog/2021/score/#probability-flow-ode" rel="external nofollow noopener" target="_blank">score-based diffusion models</a>. In score-based diffusion models, it is shown that each SDE has an associated probability flow ODE sharing the same marginal \(q_t\) (also see the Eq. (13) in <a href="https://arxiv.org/abs/2011.13456" rel="external nofollow noopener" target="_blank">Song et. al., 2021</a>). Simply using this result, we can convert the Langevin SDE to the associated ODE, \begin{aligned} \mathrm{d} \mathbf{x}_t = \Big [\nabla_\mathbf{x} \log p(\mathbf{x}_t)- \nabla_\mathbf{x} \log q_t(\mathbf{x}_t)\Big]\mathrm{d}t. \end{aligned} The marginal laws of the probability flow ODE also follow the same Fokker-Planck equation. The probability flow ODE differs from Langevin dynamics only in the nature of particle evolution: the former is deterministic, while the latter is stochastic. Similarly, using the Euler method to discretize the ODE gives \begin{align} \mathbf{x}_{i+1} \leftarrow \mathbf{x}_{i} + \epsilon \big[\nabla_{\mathbf{x}} \log {p(\mathbf{x}_i)}- \nabla_{\mathbf{x}} \log {q_{i}({\mathbf{x}_i})}\big], \quad i=0, 1, 2\cdots \end{align} It is worth noting the vector field of the probability flow ODE is the gradient of the log density ratio \(\log \big[p(\mathbf{x}_i) / q_{i}({\mathbf{x}_i})\big]\). If we have access to the log density ratio, we can use the Euler method to simulate the ODE. Next, we will discuss a method to obtain the log density ratio, analogous to training the discriminator in GANs.</p> <h4 id="22-estimating-the-vector-field-via-the-binary-classification">2.2. Estimating the vector field via the binary classification</h4> <p>Recall that in GANs, we train a discriminator to solve the following binary classification problem, \begin{aligned} \max_D \qquad \mathbb{E}_{p} \big[\log D(\mathbf{x})\big]+ \mathbb{E}_{q_i} \big[\log (1-D(\mathbf{x}))\big]. \end{aligned} The optimal discriminator is given by (see Proposition 1 in <a href="https://arxiv.org/abs/1406.2661" rel="external nofollow noopener" target="_blank">Goodfellow et. al. 2014</a>), \begin{aligned} D^{*}(\mathbf{x}) = \frac{p(\mathbf{x})}{p(\mathbf{x}) + q_i(\mathbf{x})} \end{aligned} Since the last layer of the discriminator \(D^{*}(\mathbf{x})\) is activated by the Sigmoid function \(\sigma(\cdot)\), inversing the Sigmoid activation gives the log density ratio, \begin{aligned} \sigma^{-1}\big(D^{*}(\mathbf{x})\big) =\log \frac{p(\mathbf{x})}{q_{i}({\mathbf{x}})}, \end{aligned} \(\sigma^{-1}\big(D^{*}(\mathbf{x})\big)\) is called the logit output of a binary classifier.</p> <p>This gives us a strategy for sampling via the probability flow ODE with bi-level optimization which is similar to training GANs,</p> <ul> <li>Training the discriminator with a few steps of gradient update using samples from \(q_i\) and \(p\).</li> <li>Computing the gradient of the logit output and updating particles using Eq. (1).</li> </ul> <p>In the conventional GAN theory, training the discriminator was designed to estimate the Jensen-Shannon divergence (JSD), however, the smoothness between the estimated JSD and the generator’s distribution does not exist in practice because the divergence is reconstructed using samples. One can imagine that the Monte Carlo evluation of the binary cross entropy involves a discrete step–sampling. This is the major reason causing the inconsistency between the GAN theory and its practical algorithms, e.g., the non-saturated tricks. The essential information we obtained in training the discriminator is not the JSD, but is the density ratio function which indicates the vector fields.</p> <h2 id="3-modifying-the-code-of-gans">3. Modifying the code of GANs</h2> <ul> <li>The first step is to remove the generator <code class="language-plaintext highlighter-rouge">G(z)</code>, instead we initialize \(512\) particles <code class="language-plaintext highlighter-rouge">xs</code> and put them into the <code class="language-plaintext highlighter-rouge">SGD</code> optimizer corresponding to the Euler discretization of the ODE. One can also use <code class="language-plaintext highlighter-rouge">optim.Adam([xs], lr = 0.001)</code> to incorporate the momentum of gradients. Note that we use the loss function criterion <code class="language-plaintext highlighter-rouge">nn.BCEWithLogitsLoss()</code>, this criterion directly works with the logit output of a binary classifier where <code class="language-plaintext highlighter-rouge">D_logit</code> is a MLP in this example.</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">data_dim = 2
</span># loss
<span class="p">criterion = nn.BCEWithLogitsLoss()
</span><span class="err">
</span># build network
<span class="p">D_logit = Discriminator().to(device)
</span><span class="gd">-G = Generator().to(device)
</span><span class="gi">+xs = make_8gaussians(n_samples=512).to(device)
+xs.requires_grad = True
</span><span class="err">
</span># optimizer
<span class="p">D_optimizer = optim.Adam(D_logit.parameters(), lr = 0.001)
</span><span class="gd">-G_optimizer = optim.Adam(G.parameters(), lr = 0.001)
</span><span class="gi">+G_optimizer = optim.SGD([xs], lr = 0.001)
</span></code></pre></div></div> <ul> <li>The next step is to modify the training procedure of GANs. There is no difference on training the discriminator, we just replace fake samples with a minibatch of <code class="language-plaintext highlighter-rouge">xs</code> and use a single step gradient descent to train the discriminator. The <code class="language-plaintext highlighter-rouge">G_loss</code> is now changed to <code class="language-plaintext highlighter-rouge">-torch.sum(D_logit(xs))</code> with the purpose of computing the per particle gradient. This is because there is no explicit way to perform batch gradient operation by backpropagating the loss in PyTorch, we can instead sum all per particle forward pass together to generate a scalar and backpropagating this scalar would give each particle its own gradient. An alternative is to use <code class="language-plaintext highlighter-rouge">torch.autograd</code> class but it could be more complicated.</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#==============Train the discriminator===============#
<span class="p">D_optimizer.zero_grad()
</span># eval real loss on p
<span class="p">x_real, y_real = x.view(-1, data_dim), torch.ones(batch_size, 1).to(device)
real_loss = criterion(D_logit(x_real), y_real)
</span><span class="err">
</span># eval fake loss on q_i
<span class="gd">-z = torch.randn(batch_size, z_dim).to(device)
-x_fake, y_fake = G(z), torch.zeros(batch_size, 1).to(device)
</span># randomly select some particles to train the discriminator
<span class="gi">+idx = np.random.choice(xs.shape[0], batch_size, replace=False)
+x_fake, y_fake = xs[idx], torch.zeros(batch_size, 1).to(device)
</span><span class="p">fake_loss = criterion(D_logit(x_fake), y_fake)
</span><span class="err">
</span># gradient backprop &amp; optimize ONLY D's parameters
<span class="p">D_loss = real_loss + fake_loss
D_loss.backward()
D_optimizer.step()
</span><span class="err">
</span>#==============Update particles===============#
<span class="p">G_optimizer.zero_grad()
</span><span class="gd">-z = torch.randn(batch_size, z_dim).to(device)
-x_fake, y_fake = G(z), torch.ones(batch_size, 1).to(device)
-G_loss = criterion(D_logit(x_fake), y_fake) # non-saturate trick
</span># update all particles
# allow for the batch gradient for each particle
<span class="gi">+G_loss = -torch.sum(D_logit(xs))
</span><span class="p">G_loss.backward()
G_optimizer.step()
</span></code></pre></div></div> <p>So, that’s it! We have deleted 7 lines and added 6 lines. Now we can run the code to simulate the probability flow ODE.</p> <p>The full code for the S curve is available at <a target="_blank" href="https://colab.research.google.com/github/mingxuan-yi/prob_flow_ode/blob/main/scurve_prob_ode.ipynb" rel="external nofollow noopener"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" style="width: 15%; height: auto;"> </a>.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 d-flex flex-column align-items-center"> <img src="/assets/img/blog_pic/s_sgd.gif" class="img-fluid rounded z-depth-1" style="width: 70%; height: auto;"> <figcaption style="text-align: center; margin-top: 10px;"> Using the SGD optimizer.</figcaption> </div> <div class="col-sm mt-3 mt-md-0 d-flex flex-column align-items-center"> <img src="/assets/img/blog_pic/s_adam.gif" class="img-fluid rounded z-depth-1" style="width: 70%; height: auto;"> <figcaption style="text-align: center; margin-top: 10px;"> Using the Adam optimizer. </figcaption> </div> <div> </div> </div> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"mingxuan-yi/mingxuan-yi.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Mingxuan Yi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>